{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e71131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Volatility-Integrated LSTM and GRU Models for Heston-Nandi and Component Heston-Nandi\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Volatility-Integrated GRU Model for Heston-Nandi\n",
    "\n",
    "Minimal TensorFlow 2 implementation:\n",
    "- Single-layer GRU with hidden size 1\n",
    "- Heston–Nandi variance enters as a dynamic bias in the GRU update gate\n",
    "- Joint MLE over HN parameters and GRU weights using the HN-style likelihood\n",
    "\n",
    "This is designed as a compact starting point; extensions (CHN, LSTM, multi-layer, options) can be added later.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class VolIntegratedGRU(tf.keras.Model):\n",
    "    \"\"\"Single-asset, single-layer GRU with scalar hidden state.\n",
    "\n",
    "    The Heston–Nandi variance h_t enters the GRU update gate as a dynamic bias.\n",
    "    The hidden state η_t is interpreted as the conditional variance used in the\n",
    "    return likelihood:\n",
    "\n",
    "        R_t | η_t ~ N(λ * η_t, η_t)\n",
    "\n",
    "    The Heston–Nandi recursion updates h_t using the same shock that drives\n",
    "    returns, but with η_t in the inversion as in the proposal's NN-HN variant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, dtype: tf.dtypes.DType = tf.float32):\n",
    "        super().__init__(dtype=dtype)\n",
    "\n",
    "        # === Heston–Nandi parameters (trainable scalars) ===\n",
    "        # Re-parameterised form: h_{t+1} = ω + ϕ(h_t - ω) + α(z_t^2 - 1 - 2 γ sqrt(h_t) z_t)\n",
    "        self.omega = self.add_weight(\n",
    "            name=\"omega\", shape=(), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\", shape=(), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "        self.phi = self.add_weight(\n",
    "            name=\"phi\", shape=(), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "        self.lam = self.add_weight(\n",
    "            name=\"lam\", shape=(), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "        self.gam = self.add_weight(\n",
    "            name=\"gam\", shape=(), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "\n",
    "        # === GRU parameters (hidden size = 1) ===\n",
    "        self.input_dim = int(input_dim)\n",
    "\n",
    "        # reset gate r_t\n",
    "        self.W_r = self.add_weight(\n",
    "            name=\"W_r\", shape=(self.input_dim, 1), initializer=\"glorot_uniform\", dtype=dtype\n",
    "        )\n",
    "        self.U_r = self.add_weight(\n",
    "            name=\"U_r\", shape=(1, 1), initializer=\"glorot_uniform\", dtype=dtype\n",
    "        )\n",
    "        self.b_r = self.add_weight(\n",
    "            name=\"b_r\", shape=(1,), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "\n",
    "        # update gate u_t with dynamic bias b_t = gamma_u * h_t\n",
    "        self.W_u = self.add_weight(\n",
    "            name=\"W_u\", shape=(self.input_dim, 1), initializer=\"glorot_uniform\", dtype=dtype\n",
    "        )\n",
    "        self.U_u = self.add_weight(\n",
    "            name=\"U_u\", shape=(1, 1), initializer=\"glorot_uniform\", dtype=dtype\n",
    "        )\n",
    "        self.b_u = self.add_weight(\n",
    "            name=\"b_u\", shape=(1,), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "        self.gamma_u = self.add_weight(  # scalar scale for dynamic bias\n",
    "            name=\"gamma_u\", shape=(), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "\n",
    "        # candidate state n_t\n",
    "        self.W_n = self.add_weight(\n",
    "            name=\"W_n\", shape=(self.input_dim, 1), initializer=\"glorot_uniform\", dtype=dtype\n",
    "        )\n",
    "        self.U_n = self.add_weight(\n",
    "            name=\"U_n\", shape=(1, 1), initializer=\"glorot_uniform\", dtype=dtype\n",
    "        )\n",
    "        self.b_n = self.add_weight(\n",
    "            name=\"b_n\", shape=(1,), initializer=\"zeros\", dtype=dtype\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def eps(self) -> tf.Tensor:\n",
    "        return tf.constant(1e-8, dtype=self.dtype)\n",
    "\n",
    "    def gru_step(self, x_t: tf.Tensor, h_vol_t: tf.Tensor, h_prev: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"One GRU step with volatility-integrated update gate.\n",
    "\n",
    "        Shapes (single asset):\n",
    "            x_t:      [1, input_dim]\n",
    "            h_vol_t:  [1, 1]   (Heston–Nandi variance h_t)\n",
    "            h_prev:   [1, 1]   (previous hidden state)\n",
    "        Returns:\n",
    "            eta_t:    [1, 1]   (interpreted as variance, forced positive)\n",
    "        \"\"\"\n",
    "\n",
    "        # reset gate r_t\n",
    "        r_t = tf.sigmoid(\n",
    "            tf.matmul(x_t, self.W_r) + self.b_r + tf.matmul(h_prev, self.U_r)\n",
    "        )\n",
    "\n",
    "        # update gate u_t with dynamic bias b_t = gamma_u * h_vol_t\n",
    "        b_t = self.gamma_u * h_vol_t\n",
    "        u_t = tf.sigmoid(\n",
    "            tf.matmul(x_t, self.W_u) + self.b_u + tf.matmul(h_prev, self.U_u) + b_t\n",
    "        )\n",
    "\n",
    "        # candidate state n_t\n",
    "        n_t = tf.tanh(\n",
    "            tf.matmul(x_t, self.W_n) + self.b_n + r_t * (tf.matmul(h_prev, self.U_n))\n",
    "        )\n",
    "\n",
    "        h_t = (1.0 - u_t) * n_t + u_t * h_prev\n",
    "\n",
    "        # Interpret hidden state as conditional variance η_t, enforce positivity via softplus\n",
    "        eta_t = tf.nn.softplus(h_t)\n",
    "        return eta_t\n",
    "\n",
    "    def forward(self, y: tf.Tensor, rv: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"Run joint HN–GRU recursion and return standardized shocks and variances.\n",
    "\n",
    "        Args\n",
    "        -----\n",
    "        y  : shape [T]\n",
    "            Excess returns time series.\n",
    "        rv : shape [T]\n",
    "            Realized variance time series.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        z_val : shape [T]\n",
    "            Standardized shocks z_t = (y_t - λ η_t) / sqrt(η_t).\n",
    "        h_val : shape [T]\n",
    "            Variance path η_t used in the likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        y = tf.convert_to_tensor(y, dtype=self.dtype)\n",
    "        rv = tf.convert_to_tensor(rv, dtype=self.dtype)\n",
    "\n",
    "        T = tf.shape(y)[0]\n",
    "        z_array = tf.TensorArray(self.dtype, size=T)\n",
    "        h_array = tf.TensorArray(self.dtype, size=T)\n",
    "\n",
    "        # Initial variance: simple empirical variance of returns\n",
    "        h_vol_t = tf.math.reduce_variance(y)\n",
    "        eta_t = h_vol_t\n",
    "        h_prev = tf.reshape(eta_t, (1, 1))\n",
    "\n",
    "        # t = 0 step\n",
    "        z_t = (y[0] - self.lam * eta_t) / tf.sqrt(eta_t)\n",
    "        z_array = z_array.write(0, z_t)\n",
    "        h_array = h_array.write(0, h_vol_t)\n",
    "\n",
    "        # Iterate for t = 1, ..., T-1\n",
    "        for t in tf.range(1, T):\n",
    "            y_t = y[t]\n",
    "            rv_t = rv[t]\n",
    "\n",
    "            # Heston–Nandi variance update for h_{t}\n",
    "            term = tf.square(z_t) - 1.0 - 2.0 * self.gam * tf.sqrt(h_vol_t + self.eps) * z_t\n",
    "            h_vol_t = self.omega + self.phi * (h_vol_t - self.omega) + self.alpha * term\n",
    "            h_vol_t = tf.nn.softplus(h_vol_t)\n",
    "\n",
    "            # Build GRU input x_t = [η_{t-1}, y_t, rv_t, h_vol_t]\n",
    "            x_t = tf.stack([eta_t, y_t, rv_t, h_vol_t])\n",
    "            x_t = tf.reshape(x_t, (1, self.input_dim))\n",
    "\n",
    "            # One GRU step → η_t (variance), using dynamic bias from h_vol_t\n",
    "            h_vol_mat = tf.reshape(h_vol_t, (1, 1))\n",
    "            eta_mat = self.gru_step(x_t, h_vol_mat, h_prev)\n",
    "            eta_t = tf.reshape(eta_mat, ())\n",
    "            h_prev = eta_mat\n",
    "\n",
    "            # Standardized shock using η_t\n",
    "            z_t = (y_t - self.lam * eta_t) / tf.sqrt(eta_t + self.eps)\n",
    "\n",
    "            z_array = z_array.write(t, z_t)\n",
    "            h_array = h_array.write(t, eta_t)\n",
    "\n",
    "        z_val = z_array.stack()\n",
    "        h_val = h_array.stack()\n",
    "        return z_val, h_val\n",
    "\n",
    "    def neg_loglike(\n",
    "        self,\n",
    "        z_val: tf.Tensor,\n",
    "        h_val: tf.Tensor,\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Negative log-likelihood given standardized shocks and variances.\n",
    "\n",
    "        We assume return dynamics of the form\n",
    "\n",
    "            R_t = λ η_t + sqrt(η_t) z_t,   z_t ~ N(0, 1).\n",
    "\n",
    "        The log-likelihood per observation is\n",
    "\n",
    "            -1/2 [ log(2π) + log(η_t) + z_t^2 ].\n",
    "        \"\"\"\n",
    "\n",
    "        z_val = tf.convert_to_tensor(z_val, dtype=self.dtype)\n",
    "        h_val = tf.convert_to_tensor(h_val, dtype=self.dtype)\n",
    "\n",
    "        log2pi = tf.math.log(2.0 * tf.constant(3.141592653589793, dtype=self.dtype))\n",
    "        nll = 0.5 * tf.reduce_sum(log2pi + tf.math.log(h_val + self.eps) + tf.square(z_val))\n",
    "        return nll\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_epoch(\n",
    "    model: VolIntegratedGRU,\n",
    "    optimizer: tf.keras.optimizers.Optimizer,\n",
    "    returns: tf.Tensor,\n",
    "    rv: tf.Tensor,\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"One full-sequence gradient step (no batching) for SPX.\n",
    "\n",
    "    This does MLE over the entire available history each call using the\n",
    "    joint HN–GRU recursion.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        z_val, h_val = model.forward(returns, rv)\n",
    "        loss = model.neg_loglike(z_val, h_val)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "__all__ = [\"VolIntegratedGRU\", \"train_epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536907f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 20:42:36.061631: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-11-13 20:42:36.061661: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-11-13 20:42:36.061668: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763091756.061685 1415250 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1763091756.061708 1415250 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 11879.858398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from vol_integrate_gru import VolIntegratedGRU, train_epoch\n",
    "\n",
    "# Load returns\n",
    "df_ret = pd.read_csv(\"returns.csv\")\n",
    "df_rv  = pd.read_csv(\"RV5.csv\")\n",
    "\n",
    "returns_tf = tf.convert_to_tensor(df_ret[\"exret\"].values, dtype=tf.float32)\n",
    "rv_tf      = tf.convert_to_tensor(df_rv[\"rv5\"].values, dtype=tf.float32)\n",
    "\n",
    "model = VolIntegratedGRU(input_dim=4, dtype=tf.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(500):\n",
    "    loss = train_epoch(model, optimizer, returns_tf, rv_tf)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.numpy():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced58147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7559,)\n",
      "(5017,)\n"
     ]
    }
   ],
   "source": [
    "print(returns_tf.shape)\n",
    "print(rv_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34850ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yawasante/Documents/Doctrate/Thesis/Python/first_project/Simplied_BM/SV_models.py:315: UserWarning: Feller condition violated: 2κθ = 0.6356 <= σ² = 3.4207\n",
      "  warnings.warn(f\"Feller condition violated: 2κθ = {feller_condition:.4f} <= σ² = {self.sigma**2:.4f}\")\n",
      "/Users/yawasante/Documents/Doctrate/Thesis/Python/first_project/Simplied_BM/SV_models.py:315: UserWarning: Feller condition violated: 2κθ = 0.3353 <= σ² = 1.0589\n",
      "  warnings.warn(f\"Feller condition violated: 2κθ = {feller_condition:.4f} <= σ² = {self.sigma**2:.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved P-eval artifacts to artifacts/benchmark_eval/20250919-193730/\n",
      "Heston (portfolio) MV RMSE=0.036396  |  LRM RMSE=0.006590\n",
      "Bates  (portfolio) MV RMSE=0.050755  |  LRM RMSE=0.007781\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def forward(self, y: tf.Tensor, rv: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Heston–Nandi variance recursion update with softplus positivity enforcement.\n",
    "\n",
    "        h_vol_t: current variance (shape [1,1])\n",
    "        z_t: shock at time t (shape [1,1])\n",
    "        y: returns data\n",
    "        rv: realized variance data\n",
    "        \"\"\"\n",
    "        T = tf.size(y)  # returns data is a 1D vector if 5000\n",
    "        h_vol, z_val = tf.zeros(T), tf.zeros(T)\n",
    "        \n",
    "        h_vol_t = tf.var(y)  # starting variance\n",
    "        eta_t = h_vol_t   # start with the GARCH-type volatility\n",
    "        z_t = (y[0] - self.lam * eta_t) / tf.sqrt(eta_t)\n",
    "        h_vol[t] = h_vol_t\n",
    "        z_val[t] = z_t\n",
    "        \n",
    "        for t in tf.range(1, T):\n",
    "            X_t = tf.stack(eta_t, y[t], rv[t], h_vol_t)\n",
    "            h_vol_t = self.omega + self.phi * (h_vol_t - self.omega) + self.alpha * (tf.square(z_t) - 1.0 - 2.0 * self.gam * tf.sqrt(h_vol_t) * z_t)\n",
    "            eta_t = self.gru_step(X_t, h_vol_t)  # call vol integrated GRU to get eta_t\n",
    "            z_t = (y[t] - self.lam * eta_t) / tf.sqrt(eta_t)\n",
    "            h_vol[t] = h_vol_t\n",
    "            z_val[t] = z_t\n",
    "            \n",
    "        return z_val, h_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721417b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yawasante/Documents/Doctrate/Thesis/Python/first_project/Simplied_BM/SV_models.py:315: UserWarning: Feller condition violated: 2κθ = 0.6356 <= σ² = 3.4207\n",
      "  warnings.warn(f\"Feller condition violated: 2κθ = {feller_condition:.4f} <= σ² = {self.sigma**2:.4f}\")\n",
      "/Users/yawasante/Documents/Doctrate/Thesis/Python/first_project/Simplied_BM/SV_models.py:315: UserWarning: Feller condition violated: 2κθ = 0.3353 <= σ² = 1.0589\n",
      "  warnings.warn(f\"Feller condition violated: 2κθ = {feller_condition:.4f} <= σ² = {self.sigma**2:.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved P-eval artifacts to artifacts/benchmark_eval/20250919-211159/\n",
      "Heston (portfolio) MV RMSE=0.036155  |  LRM RMSE=0.035266\n",
      "Bates  (portfolio) MV RMSE=0.050477  |  LRM RMSE=0.047679\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal, readable quadratic-hedging benchmark (Heston & Bates)\n",
    "\n",
    "What this file does (only):\n",
    "1) Load calibrated params (oneDheston.json, oneDbates.json).\n",
    "2) Build ONE real-data case: S0 from JSON, T=59/252, daily hedging (n_steps=59),\n",
    "   strikes = [3225, 2600] with types ['call','put'].\n",
    "3) Train time-step hedge ratios θ_k by sequential projection\n",
    "   - MVH: simulate under VOMM\n",
    "   - LRM: simulate under MMM\n",
    "   (all in *discounted* units X_t = e^{-(r-q)t} S_t)\n",
    "4) Evaluate under P (discounted)\n",
    "   - MVH: V̄_T = v0 + ∑ θ_k ΔX_k with v0 := E[H~ - gains]  (same rule for Heston/Bates)\n",
    "   - LRM: V̄_T = C_T + ∑ θ_k ΔX_k where C_T = H0 + L_T and\n",
    "           L_T := H~ - H0 - ∑ θ_k ΔM_k with ΔM_k = ΔX_k - E[ΔX_k] (batch-wise de-drift)\n",
    "5) Save only portfolio-level metrics & thetas in artifacts/benchmark_eval/<ts>/\n",
    "\n",
    "Notes:\n",
    "- Uses powers-of-two path counts (Sobol-friendly). We still call the model’s own RNG;\n",
    "  if your SV_models uses Sobol for QMC, it will benefit directly.\n",
    "- No jump-impact analysis, no multi-asset scaffolding, no ODE/Riccati code here.\n",
    "- Keeps names short and descriptive.\n",
    "\n",
    "Author: refactor for clarity/accuracy\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, os, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SV_models import HestonModel, BatesModel  # must provide: simulate_mmm, simulate_vomm, simulate_under_P\n",
    "\n",
    "\n",
    "# ------------------------------- helpers -------------------------------------\n",
    "\n",
    "def _next_pow2(n: int) -> int:\n",
    "    n = int(max(1, n))\n",
    "    return 1 << (n - 1).bit_length()\n",
    "\n",
    "def _disc_grid(r: float, q: float, T: float, n_steps: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    t = np.linspace(0.0, T, n_steps + 1)\n",
    "    disc = np.exp(-(r - q) * t)\n",
    "    return t, disc\n",
    "\n",
    "def _option_payoff(S_T: np.ndarray, K: float, kind: str) -> np.ndarray:\n",
    "    if kind.lower() == \"call\":\n",
    "        return np.maximum(S_T - K, 0.0)\n",
    "    elif kind.lower() == \"put\":\n",
    "        return np.maximum(K - S_T, 0.0)\n",
    "    raise ValueError(\"kind must be 'call' or 'put'\")\n",
    "\n",
    "def _batch_discounted_payoffs(S_T: np.ndarray, Ks: np.ndarray, kinds: List[str], disc_T: float, S0: float) -> np.ndarray:\n",
    "    # Normalize strikes by S0 for discounted/normalized payoff computation\n",
    "    Ks_norm = Ks / S0\n",
    "    out = np.zeros((len(Ks), S_T.shape[0]), dtype=np.float64)\n",
    "    for i, (K, knd) in enumerate(zip(Ks_norm, kinds)):\n",
    "        out[i] = disc_T * _option_payoff(S_T, K, knd)\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------- data / test case --------------------------------\n",
    "\n",
    "def _load_json(path: str) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _first(d: dict, key: str, alt: str = \"\", default=None):\n",
    "    if key in d:\n",
    "        v = d[key]\n",
    "        return v[0] if isinstance(v, list) else v\n",
    "    if alt and alt in d:\n",
    "        v = d[alt]\n",
    "        return v[0] if isinstance(v, list) else v\n",
    "    return default\n",
    "\n",
    "def _normalize_heston(d: dict) -> dict:\n",
    "    return {\n",
    "        \"S0\":    _first(d, \"S0\", \"S0_list\"),\n",
    "        \"v0\":    _first(d, \"v0\", \"v0_list\"),\n",
    "        \"r\":     _first(d, \"r\", \"r_list\"),\n",
    "        \"q\":     _first(d, \"q\", \"q_list\"),\n",
    "        \"kappa\": _first(d, \"kappa\", \"kappa_list\"),\n",
    "        \"theta\": _first(d, \"theta\", \"theta_list\"),\n",
    "        \"sigma\": _first(d, \"sigma\", \"sigma_list\"),\n",
    "        \"rho\":   _first(d, \"rho\", \"rho_list\"),\n",
    "        \"xi_s\":  _first(d, \"xi_s\", \"xi_s_list\", 0.0),\n",
    "    }\n",
    "\n",
    "def _normalize_bates(d: dict) -> dict:\n",
    "    p = _normalize_heston(d)\n",
    "    p.update({\n",
    "        \"lambda_j\": _first(d, \"lambda_j\", default=0.0),\n",
    "        \"mu_j\": _first(d, \"mu_j\", default=0.0),\n",
    "        \"sigma_j\": _first(d, \"sigma_j\", default=0.0),\n",
    "        \"E_J_minus_1\": _first(d, \"E_J_minus_1\", default=0.0),\n",
    "        \"drift_adjustment\": _first(d, \"drift_adjustment\", default=0.0),\n",
    "    })\n",
    "    return p\n",
    "\n",
    "def build_real_case(heston_json=\"oneDheston.json\", bates_json=\"oneDbates.json\"):\n",
    "    h_raw = _load_json(heston_json) if os.path.exists(heston_json) else {}\n",
    "    b_raw = _load_json(bates_json) if os.path.exists(bates_json) else {}\n",
    "    h = _normalize_heston(h_raw) if h_raw else None\n",
    "    b = _normalize_bates(b_raw) if b_raw else None\n",
    "    S0 = (h or b)[\"S0\"]\n",
    "    V0 = (h or b)[\"v0\"]\n",
    "    r, q = (h or b)[\"r\"], (h or b)[\"q\"]\n",
    "    # real-data config\n",
    "    T = 59.0 / 252.0\n",
    "    n_steps = 59\n",
    "    Ks = np.array([3225.0, 2600.0], dtype=float)\n",
    "    kinds = [\"call\", \"put\"]\n",
    "    return (h, b, dict(S0=S0, V0=V0, r=r, q=q, T=T, n_steps=n_steps, Ks=Ks, kinds=kinds))\n",
    "\n",
    "\n",
    "# ----------------------------- core hedger -----------------------------------\n",
    "\n",
    "@dataclass\n",
    "class SimpleBench:\n",
    "    name: str\n",
    "    model: object  # HestonModel or BatesModel\n",
    "    r: float\n",
    "    q: float\n",
    "\n",
    "    # ---------- simulation wrapper (returns discounted X, dX and discounted payoffs) ----------\n",
    "    def _sim_discounted(self, measure: str, S0: float, V0: float, T: float, n_steps: int,\n",
    "                        n_paths: int, Ks: np.ndarray, kinds: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        dt = T / n_steps\n",
    "        if measure == \"mmm\":\n",
    "            S, V, _ = self.model.simulate_mmm(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        elif measure == \"vomm\":\n",
    "            S, V, _ = self.model.simulate_vomm(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        elif measure == \"p\":\n",
    "            S, V, _ = self.model.simulate_under_P(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        else:\n",
    "            raise ValueError(\"measure must be 'mmm', 'vomm', or 'p'\")\n",
    "        # Normalize simulated prices by S0\n",
    "        S = S / S0\n",
    "        t, disc = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        X = (S * disc[None, :]).astype(np.float64)          # discounted price paths\n",
    "        dX = X[:, 1:] - X[:, :-1]                           # discounted increments\n",
    "        H_tilde = _batch_discounted_payoffs(S[:, -1], Ks, kinds, disc[-1], S0)\n",
    "        return X, dX, H_tilde\n",
    "\n",
    "    # ---------- LSM helpers (basis + continuation values) ----------\n",
    "    def _poly_features(self, Xk: np.ndarray, Vk: Optional[np.ndarray], tk: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build simple polynomial basis in discounted (normalized) state at time k.\n",
    "        Xk: (n_paths,) discounted normalized price; Vk: (n_paths,) variance (can be None); tk: scalar in [0, T].\n",
    "        \"\"\"\n",
    "        if Vk is None:\n",
    "            return np.column_stack([\n",
    "                np.ones_like(Xk),\n",
    "                Xk, Xk**2,\n",
    "                tk*np.ones_like(Xk),\n",
    "                tk*Xk\n",
    "            ]).astype(np.float64)\n",
    "        return np.column_stack([\n",
    "            np.ones_like(Xk),\n",
    "            Xk, Vk,\n",
    "            Xk**2, Vk**2, Xk*Vk,\n",
    "            tk*np.ones_like(Xk),\n",
    "            tk*Xk, tk*Vk\n",
    "        ]).astype(np.float64)\n",
    "\n",
    "    def _discount_normalize_from_S(self, S: np.ndarray, S0: float, T: float, n_steps: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Given raw S paths (n_paths, n_steps+1), return (X, dX, disc) with\n",
    "        X_t = e^{-(r-q)t} S_t / S0 and dX = ΔX.\"\"\"\n",
    "        t, disc = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        S_norm = S / S0\n",
    "        X = (S_norm * disc[None, :]).astype(np.float64)\n",
    "        dX = X[:, 1:] - X[:, :-1]\n",
    "        return X, dX, disc\n",
    "    \n",
    "    def _lsm_continuation_values(self, X: np.ndarray, V: Optional[np.ndarray],\n",
    "                                 H_tilde_port: np.ndarray, t_grid: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Least–squares Monte Carlo estimate of continuation value E[H~ | F_k] at each time k.\n",
    "        Returns Vhat with shape (n_paths, n_steps+1).\n",
    "        \"\"\"\n",
    "        n_paths, T_plus_1 = X.shape\n",
    "        Vhat = np.zeros((n_paths, T_plus_1), dtype=np.float64)\n",
    "\n",
    "        # terminal anchor\n",
    "        Vhat[:, -1] = H_tilde_port\n",
    "\n",
    "        # ridge for numerical stability\n",
    "        ridge = 1e-10\n",
    "\n",
    "        # backward regressions of H~ on basis at each time k\n",
    "        for k in range(T_plus_1 - 1):\n",
    "            # forward (independent) fits at time k; we regress H~ directly on current state\n",
    "            Xk = X[:, k]\n",
    "            Vk = None if V is None else V[:, k]\n",
    "            Phi = self._poly_features(Xk, Vk, t_grid[k])\n",
    "            # solve (Phi^T Phi + ridge I) beta = Phi^T H\n",
    "            A = Phi.T @ Phi\n",
    "            A.flat[::A.shape[0]+1] += ridge\n",
    "            beta = np.linalg.solve(A, Phi.T @ H_tilde_port)\n",
    "            Vhat[:, k] = Phi @ beta\n",
    "\n",
    "        return Vhat\n",
    "\n",
    "    # ---------- LRM theta under MMM (local projection on ΔX without drift-removal) ----------\n",
    "    def train_theta_lrm(self, S0: float, V0: float, T: float, n_steps: int,\n",
    "                        n_paths: int, Ks: np.ndarray, kinds: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Discrete-time LRM theta under MMM: θ_{k+1}^{LRM} = Cov(ΔV_{k+1}, ΔX_{k+1} | F_k) / Var(ΔX_{k+1} | F_k)\n",
    "        where V_k = E^{MMM}[H~ | F_k]. NO drift-removal here; drift is removed only at P-evaluation.\n",
    "        Returns a single portfolio-level theta of shape (T-1,).\n",
    "        \"\"\"\n",
    "        # simulate discounted normalized paths under MMM\n",
    "        X, dX, H_tilde_mat = self._sim_discounted(\"mmm\", S0, V0, T, n_steps, n_paths, Ks, kinds)\n",
    "        # portfolio payoff (call + put sum) in discounted units\n",
    "        H_tilde_port = H_tilde_mat.sum(axis=0)\n",
    "        # continuation values by LSM\n",
    "        t_grid, _disc = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        Vhat = self._lsm_continuation_values(X, None, H_tilde_port, t_grid)\n",
    "        # ∆V and θ via covariance/variance ratio (cross-sectional, NO drift-removal)\n",
    "        dV = Vhat[:, 1:] - Vhat[:, :-1]            # (n_paths, T-1)\n",
    "        theta = np.zeros(dX.shape[1], dtype=np.float64)\n",
    "        eps = 1e-12\n",
    "        for k in range(dX.shape[1]):\n",
    "            x = dX[:, k]\n",
    "            y = dV[:, k]\n",
    "            x_mean = x.mean()\n",
    "            y_mean = y.mean()\n",
    "            cov = ( (x - x_mean) * (y - y_mean) ).mean()\n",
    "            var = ( (x - x_mean)**2 ).mean() + eps\n",
    "            theta[k] = cov / var\n",
    "        return theta\n",
    "\n",
    "    # ---------- MVH theta under VOMM (GKW integrand via LSM; v0 handled at evaluation) ----------\n",
    "    def train_theta_mvh(self, S0: float, V0: float, T: float, n_steps: int,\n",
    "                        n_paths: int, Ks: np.ndarray, kinds: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Discrete-time MVH theta under VOMM: GKW integrand (ΔV-on-ΔX regression) plus Schweizer feedback term if available.\n",
    "        If the model does not expose a density martingale, falls back to ξ_k.\n",
    "        Returns portfolio-level θ with shape (T-1,).\n",
    "        \"\"\"\n",
    "        # (1) Simulate under VOMM; take density if available\n",
    "        have_with_density = hasattr(self.model, 'simulate_vomm_with_density')\n",
    "        if have_with_density:\n",
    "            S, V, Z, comp_ratio = self.model.simulate_vomm_with_density(S0, V0, T, T/n_steps, n_paths, _show_progress=False)\n",
    "        else:\n",
    "            S, V, _ = self.model.simulate_vomm(S0, V0, T, T/n_steps, n_paths, _show_progress=False)\n",
    "            Z = None\n",
    "            comp_ratio = np.zeros(n_steps, dtype=float)\n",
    "        # (2) Discount/normalize and build portfolio payoff H~\n",
    "        X, dX, disc = self._discount_normalize_from_S(S, S0, T, n_steps)\n",
    "        S_T_norm = (S / S0)[:, -1]\n",
    "        H_tilde_mat = _batch_discounted_payoffs(S_T_norm, Ks, kinds, disc[-1], S0=S0)\n",
    "        H_port = H_tilde_mat.sum(axis=0)\n",
    "        # (3) LSM continuation values V^H_k = E[H~ | F_k]\n",
    "        t_grid, _ = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        Vhat = self._lsm_continuation_values(X, None, H_port, t_grid)\n",
    "        dV = Vhat[:, 1:] - Vhat[:, :-1]\n",
    "        # (4) First the GKW integrand ξ_k by ΔV-on-ΔX regression (cross-sectional)\n",
    "        Tm1 = dX.shape[1]\n",
    "        xi = np.zeros(Tm1, dtype=np.float64)\n",
    "        eps = 1e-12\n",
    "        for k in range(Tm1):\n",
    "            x = dX[:, k]\n",
    "            y = dV[:, k]\n",
    "            x_mean = x.mean(); y_mean = y.mean()\n",
    "            cov = ((x - x_mean) * (y - y_mean)).mean()\n",
    "            var = ((x - x_mean)**2).mean() + eps\n",
    "            xi[k] = cov / var\n",
    "        # (5) Schweizer feedback using comp_ratio (if provided):\n",
    "        #     fb_k = comp_ratio[k] * ( E[V^H_k] - E[H~] - E[Σ_{i<k} θ_i ΔX_i] ), θ_k = ξ_k - fb_k\n",
    "        EH = float(np.mean(H_port))\n",
    "        theta = xi.copy()\n",
    "        if comp_ratio is not None and np.any(comp_ratio != 0.0):\n",
    "            cum_gains_mean = 0.0\n",
    "            for k in range(Tm1):\n",
    "                if k > 0:\n",
    "                    cum_gains_mean += float(np.mean(dX[:, k-1] * theta[k-1]))\n",
    "                Vk_mean = float(np.mean(Vhat[:, k]))\n",
    "                fb_k = comp_ratio[k] * (Vk_mean - EH - cum_gains_mean)\n",
    "                theta[k] = xi[k] - fb_k\n",
    "        return theta\n",
    "\n",
    "    # ---------- evaluate under P (portfolio = CALL+PUT) ----------\n",
    "    def evaluate_P(self, theta_mvh: np.ndarray, theta_lrm: np.ndarray,\n",
    "                   S0: float, V0: float, T: float, n_steps: int, n_paths: int,\n",
    "                   Ks: np.ndarray, kinds: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        P-measure evaluation in discounted units.\n",
    "        Metrics are computed from hedging residuals (L_T), not by adding L_T back into the portfolio value.\n",
    "        All in discounted, normalized units.\n",
    "        \"\"\"\n",
    "        # simulate discounted/normalized paths under P\n",
    "        X, dX, H_tilde_mat = self._sim_discounted(\"p\", S0, V0, T, n_steps, n_paths, Ks, kinds)\n",
    "        # portfolio = CALL + PUT (sum over listed options)\n",
    "        payoff_port = H_tilde_mat.sum(axis=0)                      # (n_paths,)\n",
    "        # thetas are already portfolio-level (shape (T-1,))\n",
    "        T_al = min(dX.shape[1], theta_mvh.shape[0], theta_lrm.shape[0])\n",
    "        dX = dX[:, :T_al]\n",
    "        th_mvh = theta_mvh[:T_al]\n",
    "        th_lrm = theta_lrm[:T_al]\n",
    "        dX_mean = dX.mean(axis=0, keepdims=True)\n",
    "        # ----------- MVH: hedging error is residual L_T (do not add L_T to value) -----------\n",
    "        gains_mvh = (dX * th_mvh[None, :]).sum(axis=1)             # Σ θ ΔX\n",
    "        v0_mvh = float(np.mean(payoff_port - gains_mvh))           # center under P for evaluation\n",
    "        err_mvh = payoff_port - (v0_mvh + gains_mvh)               # residual ≈ L_T\n",
    "        # ----------- LRM: error is L_T = H - H0 - Σ θ ΔM -----------\n",
    "        dX_mean = dX.mean(axis=0, keepdims=True)                   # (1, T_al)\n",
    "        dM = dX - dX_mean                                          # ΔM via cross-sectional mean\n",
    "        H0 = float(np.mean(payoff_port))                           # E_P[H~]\n",
    "        gains_M = (dM * th_lrm[None, :]).sum(axis=1)               # Σ θ ΔM\n",
    "        err_lrm = payoff_port - H0 - gains_M                       # = L_T\n",
    "        # ---------------- metrics (portfolio-level only) ----------------\n",
    "        def _metrics(e: np.ndarray, unhedged: np.ndarray) -> Dict:\n",
    "            mean = float(np.mean(e))\n",
    "            mse = float(np.mean(e**2))\n",
    "            var = max(mse - mean**2, 0.0)\n",
    "            rmse = float(np.sqrt(mse))\n",
    "            std = float(np.sqrt(var))\n",
    "            heff = 1.0 - (var / max(float(np.var(unhedged)), 1e-12))\n",
    "            return dict(MSE=mse, RMSE=rmse, Mean=mean, Std=std, HedgeEffPct=100.0*heff)\n",
    "        mv = _metrics(err_mvh, payoff_port)\n",
    "        lr = _metrics(err_lrm, payoff_port)\n",
    "        return dict(\n",
    "            measure=\"P\",\n",
    "            v0_mvh_disc=v0_mvh,\n",
    "            H0_lrm_disc=H0,\n",
    "            mv_metrics=mv,\n",
    "            lr_metrics=lr,\n",
    "            theta_mvh_port=th_mvh.tolist(),\n",
    "            theta_lrm_port=th_lrm.tolist(),\n",
    "        )\n",
    "\n",
    "# ------------------------------- runner --------------------------------------\n",
    "\n",
    "def run_realdata():\n",
    "    # load calibrated params and real case\n",
    "    h_p, b_p, case = build_real_case()\n",
    "    S0, V0, r, q, T, n_steps, Ks, kinds = case[\"S0\"], case[\"V0\"], case[\"r\"], case[\"q\"], case[\"T\"], case[\"n_steps\"], case[\"Ks\"], case[\"kinds\"]\n",
    "\n",
    "\n",
    "    # models\n",
    "    heston = HestonModel({'kappa': h_p['kappa'], 'theta': h_p['theta'], 'sigma': h_p['sigma'], 'rho': h_p['rho'],\n",
    "                          'v0': h_p['v0'], 'r': h_p['r'], 'q': h_p['q'], 'xi_s': h_p.get('xi_s', 0.0)})\n",
    "    if b_p is not None and all(k in b_p for k in ['lambda_j','mu_j','sigma_j']):\n",
    "        bates = BatesModel({'kappa': b_p['kappa'], 'theta': b_p['theta'], 'sigma': b_p['sigma'], 'rho': b_p['rho'],\n",
    "                            'v0': b_p['v0'], 'r': b_p['r'], 'q': b_p['q'], 'xi_s': b_p.get('xi_s', 0.0),\n",
    "                            'lambda_j': b_p['lambda_j'], 'mu_j': b_p['mu_j'], 'sigma_j': b_p['sigma_j'],\n",
    "                            'E_J_minus_1': b_p.get('E_J_minus_1', 0.0), 'drift_adjustment': b_p.get('drift_adjustment', 0.0)})\n",
    "    else:\n",
    "        bates = BatesModel({'kappa': h_p['kappa'], 'theta': h_p['theta'], 'sigma': h_p['sigma'], 'rho': h_p['rho'],\n",
    "                            'v0': h_p['v0'], 'r': h_p['r'], 'q': h_p['q'], 'xi_s': h_p.get('xi_s', 0.0),\n",
    "                            'lambda_j': 0.0, 'mu_j': 0.0, 'sigma_j': 0.0, 'E_J_minus_1': 0.0, 'drift_adjustment': 0.0})\n",
    "\n",
    "    # simple benches\n",
    "    hb = SimpleBench(\"Heston\", heston, r, q)\n",
    "    bb = SimpleBench(\"Bates\", bates, r, q)\n",
    "\n",
    "    # path budgets (powers of two)\n",
    "    n_train = _next_pow2(2**18)        # 262,144 (validation-speed)\n",
    "    n_eval  = _next_pow2(2**17)        # 131,072 (evaluation under P)\n",
    "\n",
    "    # train thetas (portfolio-level)\n",
    "    theta_lrm_h = hb.train_theta_lrm(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "    theta_mvh_h = hb.train_theta_mvh(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "\n",
    "    theta_lrm_b = bb.train_theta_lrm(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "    theta_mvh_b = bb.train_theta_mvh(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "\n",
    "    # evaluate under P\n",
    "    res_h = hb.evaluate_P(theta_mvh_h, theta_lrm_h, S0, V0, T, n_steps, n_eval, Ks, kinds)\n",
    "    res_b = bb.evaluate_P(theta_mvh_b, theta_lrm_b, S0, V0, T, n_steps, n_eval, Ks, kinds)\n",
    "\n",
    "    # save artifacts\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    outdir = f\"artifacts/benchmark_eval/{ts}/\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    with open(os.path.join(outdir, \"heston_eval_P.json\"), \"w\") as f:\n",
    "        json.dump(res_h, f, indent=2)\n",
    "    with open(os.path.join(outdir, \"bates_eval_P.json\"), \"w\") as f:\n",
    "        json.dump(res_b, f, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved P-eval artifacts to {outdir}\")\n",
    "    print(f\"Heston (portfolio) MV RMSE={res_h['mv_metrics']['RMSE']:.6f}  |  LRM RMSE={res_h['lr_metrics']['RMSE']:.6f}\")\n",
    "    print(f\"Bates  (portfolio) MV RMSE={res_b['mv_metrics']['RMSE']:.6f}  |  LRM RMSE={res_b['lr_metrics']['RMSE']:.6f}\")\n",
    "\n",
    "    return dict(heston=res_h, bates=res_b, outdir=outdir)\n",
    "\n",
    "\n",
    "# ------------------------------ entry point ----------------------------------\n",
    "\n",
    "def runner():\n",
    "    return run_realdata()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42589827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved P-eval artifacts to artifacts/benchmark_eval/20250919-212340/\n",
      "Heston (portfolio) MV RMSE=0.034057  |  LRM RMSE=0.038170\n",
      "Bates  (portfolio) MV RMSE=0.050461  |  LRM RMSE=0.048618\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal, readable quadratic-hedging benchmark (Heston & Bates)\n",
    "\n",
    "What this file does (only):\n",
    "1) Load calibrated params (oneDheston.json, oneDbates.json).\n",
    "2) Build ONE real-data case: S0 from JSON, T=59/252, daily hedging (n_steps=59),\n",
    "   strikes = [3225, 2600] with types ['call','put'].\n",
    "3) Train time-step hedge ratios θ_k by sequential projection\n",
    "   - MVH: simulate under VOMM\n",
    "   - LRM: simulate under MMM\n",
    "   (all in *discounted* units X_t = e^{-(r-q)t} S_t)\n",
    "4) Evaluate under P (discounted)\n",
    "   - MVH: V̄_T = v0 + ∑ θ_k ΔX_k with v0 := E[H~ - gains]  (same rule for Heston/Bates)\n",
    "   - LRM: V̄_T = C_T + ∑ θ_k ΔX_k where C_T = H0 + L_T and\n",
    "           L_T := H~ - H0 - ∑ θ_k ΔM_k with ΔM_k = ΔX_k - E[ΔX_k] (batch-wise de-drift)\n",
    "5) Save only portfolio-level metrics & thetas in artifacts/benchmark_eval/<ts>/\n",
    "\n",
    "Notes:\n",
    "- Uses powers-of-two path counts (Sobol-friendly). We still call the model’s own RNG;\n",
    "  if your SV_models uses Sobol for QMC, it will benefit directly.\n",
    "- No jump-impact analysis, no multi-asset scaffolding, no ODE/Riccati code here.\n",
    "- Keeps names short and descriptive.\n",
    "\n",
    "Author: refactor for clarity/accuracy\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, os, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SV_models import HestonModel, BatesModel  # must provide: simulate_mmm, simulate_vomm, simulate_under_P\n",
    "\n",
    "\n",
    "# ------------------------------- helpers -------------------------------------\n",
    "\n",
    "def _next_pow2(n: int) -> int:\n",
    "    n = int(max(1, n))\n",
    "    return 1 << (n - 1).bit_length()\n",
    "\n",
    "def _disc_grid(r: float, q: float, T: float, n_steps: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    t = np.linspace(0.0, T, n_steps + 1)\n",
    "    disc = np.exp(-(r - q) * t)\n",
    "    return t, disc\n",
    "\n",
    "def _option_payoff(S_T: np.ndarray, K: float, kind: str) -> np.ndarray:\n",
    "    if kind.lower() == \"call\":\n",
    "        return np.maximum(S_T - K, 0.0)\n",
    "    elif kind.lower() == \"put\":\n",
    "        return np.maximum(K - S_T, 0.0)\n",
    "    raise ValueError(\"kind must be 'call' or 'put'\")\n",
    "\n",
    "def _batch_discounted_payoffs(S_T: np.ndarray, Ks: np.ndarray, kinds: List[str], disc_T: float, S0: float) -> np.ndarray:\n",
    "    # Normalize strikes by S0 for discounted/normalized payoff computation\n",
    "    Ks_norm = Ks / S0\n",
    "    out = np.zeros((len(Ks), S_T.shape[0]), dtype=np.float64)\n",
    "    for i, (K, knd) in enumerate(zip(Ks_norm, kinds)):\n",
    "        out[i] = disc_T * _option_payoff(S_T, K, knd)\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------- data / test case --------------------------------\n",
    "\n",
    "def _load_json(path: str) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _first(d: dict, key: str, alt: str = \"\", default=None):\n",
    "    if key in d:\n",
    "        v = d[key]\n",
    "        return v[0] if isinstance(v, list) else v\n",
    "    if alt and alt in d:\n",
    "        v = d[alt]\n",
    "        return v[0] if isinstance(v, list) else v\n",
    "    return default\n",
    "\n",
    "def _normalize_heston(d: dict) -> dict:\n",
    "    return {\n",
    "        \"S0\":    _first(d, \"S0\", \"S0_list\"),\n",
    "        \"v0\":    _first(d, \"v0\", \"v0_list\"),\n",
    "        \"r\":     _first(d, \"r\", \"r_list\"),\n",
    "        \"q\":     _first(d, \"q\", \"q_list\"),\n",
    "        \"kappa\": _first(d, \"kappa\", \"kappa_list\"),\n",
    "        \"theta\": _first(d, \"theta\", \"theta_list\"),\n",
    "        \"sigma\": _first(d, \"sigma\", \"sigma_list\"),\n",
    "        \"rho\":   _first(d, \"rho\", \"rho_list\"),\n",
    "        \"xi_s\":  _first(d, \"xi_s\", \"xi_s_list\", 0.0),\n",
    "    }\n",
    "\n",
    "def _normalize_bates(d: dict) -> dict:\n",
    "    p = _normalize_heston(d)\n",
    "    p.update({\n",
    "        \"lambda_j\": _first(d, \"lambda_j\", default=0.0),\n",
    "        \"mu_j\": _first(d, \"mu_j\", default=0.0),\n",
    "        \"sigma_j\": _first(d, \"sigma_j\", default=0.0),\n",
    "        \"E_J_minus_1\": _first(d, \"E_J_minus_1\", default=0.0),\n",
    "        \"drift_adjustment\": _first(d, \"drift_adjustment\", default=0.0),\n",
    "    })\n",
    "    return p\n",
    "\n",
    "def build_real_case(heston_json=\"oneDheston.json\", bates_json=\"oneDbates.json\"):\n",
    "    h_raw = _load_json(heston_json) if os.path.exists(heston_json) else {}\n",
    "    b_raw = _load_json(bates_json) if os.path.exists(bates_json) else {}\n",
    "    h = _normalize_heston(h_raw) if h_raw else None\n",
    "    b = _normalize_bates(b_raw) if b_raw else None\n",
    "    S0 = (h or b)[\"S0\"]\n",
    "    V0 = (h or b)[\"v0\"]\n",
    "    r, q = (h or b)[\"r\"], (h or b)[\"q\"]\n",
    "    # real-data config\n",
    "    T = 59.0 / 252.0\n",
    "    n_steps = 59\n",
    "    Ks = np.array([3225.0, 2600.0], dtype=float)\n",
    "    kinds = [\"call\", \"put\"]\n",
    "    return (h, b, dict(S0=S0, V0=V0, r=r, q=q, T=T, n_steps=n_steps, Ks=Ks, kinds=kinds))\n",
    "\n",
    "\n",
    "# ----------------------------- core hedger -----------------------------------\n",
    "\n",
    "@dataclass\n",
    "class SimpleBench:\n",
    "    name: str\n",
    "    model: object  # HestonModel or BatesModel\n",
    "    r: float\n",
    "    q: float\n",
    "\n",
    "    # ---------- simulation wrapper (returns discounted X, dX and discounted payoffs) ----------\n",
    "    def _sim_discounted(self, measure: str, S0: float, V0: float, T: float, n_steps: int,\n",
    "                        n_paths: int, Ks: np.ndarray, kinds: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        dt = T / n_steps\n",
    "        if measure == \"mmm\":\n",
    "            S, V, _ = self.model.simulate_mmm(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        elif measure == \"vomm\":\n",
    "            S, V, _ = self.model.simulate_vomm(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        elif measure == \"p\":\n",
    "            S, V, _ = self.model.simulate_under_P(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        else:\n",
    "            raise ValueError(\"measure must be 'mmm', 'vomm', or 'p'\")\n",
    "        # Normalize simulated prices by S0\n",
    "        S = S / S0\n",
    "        t, disc = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        X = (S * disc[None, :]).astype(np.float64)          # discounted price paths\n",
    "        dX = X[:, 1:] - X[:, :-1]                           # discounted increments\n",
    "        H_tilde = _batch_discounted_payoffs(S[:, -1], Ks, kinds, disc[-1], S0)\n",
    "        return X, dX, H_tilde\n",
    "\n",
    "    # ---------- LSM helpers (basis + continuation values) ----------\n",
    "    def _poly_features(self, Xk: np.ndarray, Vk: Optional[np.ndarray], tk: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build simple polynomial basis in discounted (normalized) state at time k.\n",
    "        Xk: (n_paths,) discounted normalized price; Vk: (n_paths,) variance (can be None); tk: scalar in [0, T].\n",
    "        \"\"\"\n",
    "        if Vk is None:\n",
    "            return np.column_stack([\n",
    "                np.ones_like(Xk),\n",
    "                Xk, Xk**2,\n",
    "                tk*np.ones_like(Xk),\n",
    "                tk*Xk\n",
    "            ]).astype(np.float64)\n",
    "        return np.column_stack([\n",
    "            np.ones_like(Xk),\n",
    "            Xk, Vk,\n",
    "            Xk**2, Vk**2, Xk*Vk,\n",
    "            tk*np.ones_like(Xk),\n",
    "            tk*Xk, tk*Vk\n",
    "        ]).astype(np.float64)\n",
    "\n",
    "    def _cond_mean_var(self, dX_col: np.ndarray, Xk_col: np.ndarray, Vk_col: Optional[np.ndarray], tk: float) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Conditional mean/variance of ΔX_k given F_k via polynomial regression.\n",
    "        Returns:\n",
    "          mu_hat: pathwise conditional mean (n_paths,)\n",
    "          var_hat: scalar conditional variance estimate\n",
    "        \"\"\"\n",
    "        Phi = self._poly_features(Xk_col, (Vk_col if Vk_col is not None else None), tk)\n",
    "        A = Phi.T @ Phi\n",
    "        A.flat[::A.shape[0]+1] += 1e-10\n",
    "        beta = np.linalg.solve(A, Phi.T @ dX_col)\n",
    "        mu_hat = Phi @ beta\n",
    "        resid = dX_col - mu_hat\n",
    "        var_hat = float(np.mean(resid * resid))\n",
    "        return mu_hat, var_hat\n",
    "\n",
    "    def _discount_normalize_from_S(self, S: np.ndarray, S0: float, T: float, n_steps: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Given raw S paths (n_paths, n_steps+1), return (X, dX, disc) with\n",
    "        X_t = e^{-(r-q)t} S_t / S0 and dX = ΔX.\"\"\"\n",
    "        t, disc = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        S_norm = S / S0\n",
    "        X = (S_norm * disc[None, :]).astype(np.float64)\n",
    "        dX = X[:, 1:] - X[:, :-1]\n",
    "        return X, dX, disc\n",
    "     \n",
    "    def _apply_betas(self, X: np.ndarray, V: Optional[np.ndarray], t_grid: np.ndarray, betas: List[np.ndarray]) -> np.ndarray:\n",
    "      n_paths, T_plus_1 = X.shape\n",
    "      Vhat = np.zeros((n_paths, T_plus_1), dtype=np.float64)\n",
    "      for k in range(T_plus_1 - 1):\n",
    "         Xk = X[:, k]\n",
    "         Vk = None if V is None else V[:, k]\n",
    "         Phi = self._poly_features(Xk, Vk, t_grid[k])\n",
    "         Vhat[:, k] = Phi @ betas[k]\n",
    "      # terminal left at 0; we only need k<T values for dV etc.\n",
    "      return Vhat\n",
    "    \n",
    "    def _lsm_continuation_values(self, X: np.ndarray, V: Optional[np.ndarray],\n",
    "                             H_tilde_port: np.ndarray, t_grid: np.ndarray) -> Tuple[np.ndarray, List[np.ndarray]]:\n",
    "         n_paths, T_plus_1 = X.shape\n",
    "         Vhat = np.zeros((n_paths, T_plus_1), dtype=np.float64)\n",
    "         betas: List[np.ndarray] = [None]*(T_plus_1)  # store beta per time\n",
    "\n",
    "         Vhat[:, -1] = H_tilde_port\n",
    "         ridge = 1e-10\n",
    "\n",
    "         for k in range(T_plus_1 - 1):\n",
    "            Xk = X[:, k]\n",
    "            Vk = None if V is None else V[:, k]\n",
    "            Phi = self._poly_features(Xk, Vk, t_grid[k])\n",
    "            A = Phi.T @ Phi\n",
    "            A.flat[::A.shape[0]+1] += ridge\n",
    "            beta = np.linalg.solve(A, Phi.T @ H_tilde_port)\n",
    "            Vhat[:, k] = Phi @ beta\n",
    "            betas[k] = beta\n",
    "\n",
    "         betas[-1] = np.zeros_like(betas[0])  # terminal dummy\n",
    "         return Vhat, betas\n",
    "\n",
    "    # ---------- LRM theta under MMM (local projection on ΔX without drift-removal) ----------\n",
    "    def train_theta_lrm(self, S0, V0, T, n_steps, n_paths, Ks, kinds):\n",
    "         X, dX, H_tilde_mat = self._sim_discounted(\"mmm\", S0, V0, T, n_steps, n_paths, Ks, kinds)\n",
    "         H_port = H_tilde_mat.sum(axis=0)\n",
    "         t_grid, _ = _disc_grid(self.r, self.q, T, n_steps)\n",
    "         Vhat, betas = self._lsm_continuation_values(X, None, H_port, t_grid)\n",
    "\n",
    "         dV = Vhat[:, 1:] - Vhat[:, :-1]\n",
    "         theta = np.zeros(dX.shape[1], dtype=np.float64)\n",
    "         eps = 1e-12\n",
    "         for k in range(dX.shape[1]):\n",
    "            x, y = dX[:, k], dV[:, k]\n",
    "            cov = ((x - x.mean()) * (y - y.mean())).mean()\n",
    "            var = ((x - x.mean())**2).mean() + eps\n",
    "            theta[k] = cov / var\n",
    "\n",
    "         H0_lrm = float(np.mean(H_port))  # E^{MMM}[H~]\n",
    "         return theta, betas, H0_lrm\n",
    "\n",
    "    # ---------- MVH theta under VOMM (GKW integrand via LSM; v0 handled at evaluation) ----------\n",
    "    def train_theta_mvh(self, S0, V0, T, n_steps, n_paths, Ks, kinds):\n",
    "         have_with_density = hasattr(self.model, 'simulate_vomm_with_density')\n",
    "         if have_with_density:\n",
    "            S, V, Z, comp_ratio = self.model.simulate_vomm_with_density(S0, V0, T, T/n_steps, n_paths, _show_progress=False)\n",
    "         else:\n",
    "            S, V, _ = self.model.simulate_vomm(S0, V0, T, T/n_steps, n_paths, _show_progress=False)\n",
    "            Z = None; comp_ratio = None\n",
    "\n",
    "         X, dX, disc = self._discount_normalize_from_S(S, S0, T, n_steps)\n",
    "         S_T_norm = (S / S0)[:, -1]\n",
    "         H_tilde_mat = _batch_discounted_payoffs(S_T_norm, Ks, kinds, disc[-1], S0=S0)\n",
    "         H_port = H_tilde_mat.sum(axis=0)\n",
    "\n",
    "         t_grid, _ = _disc_grid(self.r, self.q, T, n_steps)\n",
    "         Vhat, betas = self._lsm_continuation_values(X, None, H_port, t_grid)\n",
    "         dV = Vhat[:, 1:] - Vhat[:, :-1]\n",
    "\n",
    "         Tm1 = dX.shape[1]\n",
    "         xi = np.zeros(Tm1, dtype=np.float64)\n",
    "         eps = 1e-12\n",
    "         for k in range(Tm1):\n",
    "            x, y = dX[:, k], dV[:, k]\n",
    "            cov = ((x - x.mean()) * (y - y.mean())).mean()\n",
    "            var = ((x - x.mean())**2).mean() + eps\n",
    "            xi[k] = cov / var\n",
    "\n",
    "         H0_mvh = float(np.mean(H_port))  # E^{VOMM}[H~]\n",
    "         return xi, betas, H0_mvh, comp_ratio\n",
    "\n",
    "    # ---------- evaluate under P (portfolio = CALL+PUT) ----------\n",
    "    def evaluate_P(self,\n",
    "               xi_mvh: np.ndarray, betas_vomm: List[np.ndarray], H0_mvh: float,\n",
    "               theta_lrm: np.ndarray, betas_mmm: List[np.ndarray], H0_lrm: float,\n",
    "               S0: float, V0: float, T: float, n_steps: int, n_paths: int,\n",
    "               Ks: np.ndarray, kinds: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Paper-faithful P-evaluation (discounted, normalized).\n",
    "        - MVH error:  H~ - (H0_mvh + sum θ^{MVH}_k(path) ΔX_k)  with feedback (Eq. 4.24)\n",
    "        - LRM error:  H~ - (H0_lrm + sum θ^{LRM}_k ΔM_k)        with ΔM_k = ΔX_k - E[ΔX_k|F_k]\n",
    "        Returns orthogonality metrics for both MVH and LRM.\n",
    "        \"\"\"\n",
    "        dt = T / n_steps\n",
    "\n",
    "        # Simulate under P and build discounted normalized state\n",
    "        S, V, _ = self.model.simulate_under_P(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        X, dX, disc = self._discount_normalize_from_S(S, S0, T, n_steps)\n",
    "\n",
    "        # Discounted payoff (portfolio = sum over options), already normalized by S0 in payoff helper\n",
    "        S_T_norm = (S / S0)[:, -1]\n",
    "        H_tilde_mat = _batch_discounted_payoffs(S_T_norm, Ks, kinds, disc[-1], S0=S0)\n",
    "        payoff_port = H_tilde_mat.sum(axis=0)  # (n_paths,)\n",
    "\n",
    "        T_al = min(dX.shape[1], xi_mvh.shape[0], theta_lrm.shape[0])\n",
    "        dX = dX[:, :T_al]\n",
    "        Xk = X[:, :T_al]              # state X at begins of steps\n",
    "        Vk = V[:, :T_al] if V is not None else None\n",
    "        t_grid, _ = _disc_grid(self.r, self.q, T, n_steps)\n",
    "\n",
    "        # ---------- LRM: build ΔM_k via conditional mean regression per time ----------\n",
    "        Phi_list = []\n",
    "        for k in range(T_al):\n",
    "            phi = np.column_stack([\n",
    "                np.ones_like(Xk[:, k]), Xk[:, k],\n",
    "                (Vk[:, k] if Vk is not None else Xk[:, k]*0.0),\n",
    "                Xk[:, k]**2,\n",
    "                (Xk[:, k]*(Vk[:, k] if Vk is not None else 0.0)),\n",
    "                (Vk[:, k] if Vk is not None else 0.0)**2\n",
    "            ]).astype(np.float64)\n",
    "            Phi_list.append(phi)\n",
    "        dM = np.empty_like(dX)\n",
    "        mu_cond = np.empty_like(dX)\n",
    "        for k in range(T_al):\n",
    "            A = Phi_list[k].T @ Phi_list[k]\n",
    "            A.flat[::A.shape[0]+1] += 1e-10\n",
    "            beta_mu = np.linalg.solve(A, Phi_list[k].T @ dX[:, k])\n",
    "            mu_hat = Phi_list[k] @ beta_mu                  # ≈ E_P[ΔX_k | F_k]\n",
    "            mu_cond[:, k] = mu_hat\n",
    "            dM[:, k] = dX[:, k] - mu_hat\n",
    "\n",
    "        # LRM: pathwise gains and error\n",
    "        gains_lrm = (dM * theta_lrm[:T_al][None, :]).sum(axis=1)\n",
    "        err_lrm = payoff_port - (H0_lrm + gains_lrm)       # equals L_T (discrete)\n",
    "\n",
    "        # ---------- LRM orthogonality: E[L_T * M_T] with M_T = Σ θ_k ΔM_k (should be ~0) ----------\n",
    "        # For LRM, orthogonality: E[L_T * M_T] where M_T = sum θ_k dM_k\n",
    "        M_T_lrm = gains_lrm\n",
    "        L_T_lrm = err_lrm\n",
    "        ortho_lrm = float(np.mean(L_T_lrm * M_T_lrm))\n",
    "\n",
    "        # ---------- MVH: feedback using conditional regressions (stable scalar α_k) ----------\n",
    "        # Reconstruct V^{H,~P}_k along P-paths using VOMM betas\n",
    "        Vhat_vomm = self._apply_betas(X[:, :T_al+1], None, t_grid, betas_vomm)\n",
    "\n",
    "        # Compute scalar α_k = E[μ_k] / Var_k with μ_k, Var_k from conditional regressions at each time\n",
    "        alpha_scalar = np.zeros(T_al, dtype=np.float64)\n",
    "        for k in range(T_al):\n",
    "            mu_hat_k, var_hat_k = self._cond_mean_var(dX[:, k], Xk[:, k], (Vk[:, k] if Vk is not None else None), t_grid[k])\n",
    "            num = float(np.mean(mu_hat_k))\n",
    "            den = max(var_hat_k, 1e-12)\n",
    "            a = num / den\n",
    "            # conservative clamp to avoid runaway recursion; keeps unbiasedness for reasonable ranges\n",
    "            alpha_scalar[k] = np.clip(a, -100.0, 100.0)\n",
    "\n",
    "        # Recursion: θ^{MVH}_k(path) = ξ_k + α_k * ( Vhat_k - H0_mvh - sum_{j<k} θ_j ΔX_j )\n",
    "        theta_path = np.zeros_like(dX)\n",
    "        cum = np.zeros(dX.shape[0], dtype=np.float64)   # cumulative gains up to k-1\n",
    "        theta_clip = 50.0\n",
    "        cum_clip = 10.0\n",
    "        for k in range(T_al):\n",
    "            Vk_meanless = Vhat_vomm[:, k] - H0_mvh\n",
    "            raw_theta = xi_mvh[k] + alpha_scalar[k] * (Vk_meanless - cum)\n",
    "            raw_theta = np.clip(raw_theta, -theta_clip, theta_clip)\n",
    "            theta_path[:, k] = raw_theta\n",
    "            cum = np.clip(cum + raw_theta * dX[:, k], -cum_clip, cum_clip)\n",
    "\n",
    "        gains_mvh = (theta_path * dX).sum(axis=1)\n",
    "        # MVH error equals cost martingale \\tilde L_T under P (HPS Eq.4.24 discrete analogue)\n",
    "        err_mvh = payoff_port - (H0_mvh + gains_mvh)\n",
    "\n",
    "        # ---------- metrics ----------\n",
    "        def _metrics(e: np.ndarray, unhedged: np.ndarray) -> Dict:\n",
    "            mean = float(np.mean(e))\n",
    "            mse = float(np.mean(e**2))\n",
    "            var = max(mse - mean**2, 0.0)\n",
    "            rmse = float(np.sqrt(mse))\n",
    "            std = float(np.sqrt(var))\n",
    "            heff = 1.0 - (var / max(float(np.var(unhedged)), 1e-12))\n",
    "            return dict(MSE=mse, RMSE=rmse, Mean=mean, Std=std, HedgeEffPct=100.0*heff)\n",
    "\n",
    "        mv = _metrics(err_mvh, payoff_port)\n",
    "        lr = _metrics(err_lrm, payoff_port)\n",
    "\n",
    "        return dict(\n",
    "            measure=\"P\",\n",
    "            v0_mvh_disc=float(H0_mvh),\n",
    "            H0_lrm_disc=float(H0_lrm),\n",
    "            mv_metrics=mv,\n",
    "            lr_metrics=lr,\n",
    "            theta_mvh_port=None,  # pathwise; too big to dump by default\n",
    "            theta_lrm_port=theta_lrm[:T_al].tolist(),\n",
    "            # orthogonality_mv=float(ortho_mvh),\n",
    "            orthogonality_lrm=float(ortho_lrm),\n",
    "        )\n",
    "\n",
    "# ------------------------------- runner --------------------------------------\n",
    "\n",
    "def run_realdata():\n",
    "    # load calibrated params and real case\n",
    "    h_p, b_p, case = build_real_case()\n",
    "    S0, V0, r, q, T, n_steps, Ks, kinds = case[\"S0\"], case[\"V0\"], case[\"r\"], case[\"q\"], case[\"T\"], case[\"n_steps\"], case[\"Ks\"], case[\"kinds\"]\n",
    "\n",
    "\n",
    "    # models\n",
    "    heston = HestonModel({'kappa': h_p['kappa'], 'theta': h_p['theta'], 'sigma': h_p['sigma'], 'rho': h_p['rho'],\n",
    "                          'v0': h_p['v0'], 'r': h_p['r'], 'q': h_p['q'], 'xi_s': h_p.get('xi_s', 0.0)})\n",
    "    if b_p is not None and all(k in b_p for k in ['lambda_j','mu_j','sigma_j']):\n",
    "        bates = BatesModel({'kappa': b_p['kappa'], 'theta': b_p['theta'], 'sigma': b_p['sigma'], 'rho': b_p['rho'],\n",
    "                            'v0': b_p['v0'], 'r': b_p['r'], 'q': b_p['q'], 'xi_s': b_p.get('xi_s', 0.0),\n",
    "                            'lambda_j': b_p['lambda_j'], 'mu_j': b_p['mu_j'], 'sigma_j': b_p['sigma_j'],\n",
    "                            'E_J_minus_1': b_p.get('E_J_minus_1', 0.0), 'drift_adjustment': b_p.get('drift_adjustment', 0.0)})\n",
    "    else:\n",
    "        bates = BatesModel({'kappa': h_p['kappa'], 'theta': h_p['theta'], 'sigma': h_p['sigma'], 'rho': h_p['rho'],\n",
    "                            'v0': h_p['v0'], 'r': h_p['r'], 'q': h_p['q'], 'xi_s': h_p.get('xi_s', 0.0),\n",
    "                            'lambda_j': 0.0, 'mu_j': 0.0, 'sigma_j': 0.0, 'E_J_minus_1': 0.0, 'drift_adjustment': 0.0})\n",
    "\n",
    "    # simple benches\n",
    "    hb = SimpleBench(\"Heston\", heston, r, q)\n",
    "    bb = SimpleBench(\"Bates\", bates, r, q)\n",
    "\n",
    "    # path budgets (powers of two)\n",
    "    n_train = _next_pow2(2**18)        # 262,144 (validation-speed)\n",
    "    n_eval  = _next_pow2(2**17)        # 131,072 (evaluation under P)\n",
    "\n",
    "    # train thetas (portfolio-level)\n",
    "    xi_lrm_h, betas_mmm_h, H0_lrm_h = hb.train_theta_lrm(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "    xi_mvh_h, betas_vomm_h, H0_mvh_h, comp_h = hb.train_theta_mvh(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "\n",
    "    xi_lrm_b, betas_mmm_b, H0_lrm_b = bb.train_theta_lrm(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "    xi_mvh_b, betas_vomm_b, H0_mvh_b, comp_b = bb.train_theta_mvh(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "\n",
    "    # evaluate under P\n",
    "    res_h = hb.evaluate_P(xi_mvh_h, betas_vomm_h, H0_mvh_h,\n",
    "                      xi_lrm_h, betas_mmm_h, H0_lrm_h,\n",
    "                      S0, V0, T, n_steps, n_eval, Ks, kinds)\n",
    "    res_b = bb.evaluate_P(xi_mvh_b, betas_vomm_b, H0_mvh_b,\n",
    "                      xi_lrm_b, betas_mmm_b, H0_lrm_b,\n",
    "                      S0, V0, T, n_steps, n_eval, Ks, kinds)\n",
    "\n",
    "    # save artifacts\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    outdir = f\"artifacts/benchmark_eval/{ts}/\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    with open(os.path.join(outdir, \"heston_eval_P.json\"), \"w\") as f:\n",
    "        json.dump(res_h, f, indent=2)\n",
    "    with open(os.path.join(outdir, \"bates_eval_P.json\"), \"w\") as f:\n",
    "        json.dump(res_b, f, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved P-eval artifacts to {outdir}\")\n",
    "    print(f\"Heston (portfolio) MV RMSE={res_h['mv_metrics']['RMSE']:.6f}  |  LRM RMSE={res_h['lr_metrics']['RMSE']:.6f}\")\n",
    "    print(f\"Bates  (portfolio) MV RMSE={res_b['mv_metrics']['RMSE']:.6f}  |  LRM RMSE={res_b['lr_metrics']['RMSE']:.6f}\")\n",
    "\n",
    "    return dict(heston=res_h, bates=res_b, outdir=outdir)\n",
    "\n",
    "\n",
    "# ------------------------------ entry point ----------------------------------\n",
    "\n",
    "def runner():\n",
    "    return run_realdata()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a657f9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved P-eval artifacts to artifacts/benchmark_eval/20250919-214921/\n",
      "Heston (portfolio) MV RMSE=0.034136  |  LRM RMSE=0.037951\n",
      "Bates  (portfolio) MV RMSE=0.050596  |  LRM RMSE=0.048986\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal, readable quadratic-hedging benchmark (Heston & Bates)\n",
    "\n",
    "What this file does (only):\n",
    "1) Load calibrated params (oneDheston.json, oneDbates.json).\n",
    "2) Build ONE real-data case: S0 from JSON, T=59/252, daily hedging (n_steps=59),\n",
    "   strikes = [3225, 2600] with types ['call','put'].\n",
    "3) Train time-step hedge ratios θ_k by sequential projection\n",
    "   - MVH: simulate under VOMM\n",
    "   - LRM: simulate under MMM\n",
    "   (all in *discounted* units X_t = e^{-(r-q)t} S_t)\n",
    "4) Evaluate under P (discounted)\n",
    "   - MVH: V̄_T = v0 + ∑ θ_k ΔX_k with v0 := E[H~ - gains]  (same rule for Heston/Bates)\n",
    "   - LRM: V̄_T = C_T + ∑ θ_k ΔX_k where C_T = H0 + L_T and\n",
    "           L_T := H~ - H0 - ∑ θ_k ΔM_k with ΔM_k = ΔX_k - E[ΔX_k] (batch-wise de-drift)\n",
    "5) Save only portfolio-level metrics & thetas in artifacts/benchmark_eval/<ts>/\n",
    "\n",
    "Notes:\n",
    "- Uses powers-of-two path counts (Sobol-friendly). We still call the model’s own RNG;\n",
    "  if your SV_models uses Sobol for QMC, it will benefit directly.\n",
    "- No jump-impact analysis, no multi-asset scaffolding, no ODE/Riccati code here.\n",
    "- Keeps names short and descriptive.\n",
    "\n",
    "Author: refactor for clarity/accuracy\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, os, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from SV_models import HestonModel, BatesModel  # must provide: simulate_mmm, simulate_vomm, simulate_under_P\n",
    "\n",
    "\n",
    "# ------------------------------- helpers -------------------------------------\n",
    "\n",
    "def _next_pow2(n: int) -> int:\n",
    "    n = int(max(1, n))\n",
    "    return 1 << (n - 1).bit_length()\n",
    "\n",
    "def _disc_grid(r: float, q: float, T: float, n_steps: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    t = np.linspace(0.0, T, n_steps + 1)\n",
    "    disc = np.exp(-(r - q) * t)\n",
    "    return t, disc\n",
    "\n",
    "def _option_payoff(S_T: np.ndarray, K: float, kind: str) -> np.ndarray:\n",
    "    if kind.lower() == \"call\":\n",
    "        return np.maximum(S_T - K, 0.0)\n",
    "    elif kind.lower() == \"put\":\n",
    "        return np.maximum(K - S_T, 0.0)\n",
    "    raise ValueError(\"kind must be 'call' or 'put'\")\n",
    "\n",
    "def _batch_discounted_payoffs(S_T: np.ndarray, Ks: np.ndarray, kinds: List[str], disc_T: float, S0: float) -> np.ndarray:\n",
    "    # Normalize strikes by S0 for discounted/normalized payoff computation\n",
    "    Ks_norm = Ks / S0\n",
    "    out = np.zeros((len(Ks), S_T.shape[0]), dtype=np.float64)\n",
    "    for i, (K, knd) in enumerate(zip(Ks_norm, kinds)):\n",
    "        out[i] = disc_T * _option_payoff(S_T, K, knd)\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------- data / test case --------------------------------\n",
    "\n",
    "def _load_json(path: str) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _first(d: dict, key: str, alt: str = \"\", default=None):\n",
    "    if key in d:\n",
    "        v = d[key]\n",
    "        return v[0] if isinstance(v, list) else v\n",
    "    if alt and alt in d:\n",
    "        v = d[alt]\n",
    "        return v[0] if isinstance(v, list) else v\n",
    "    return default\n",
    "\n",
    "def _normalize_heston(d: dict) -> dict:\n",
    "    return {\n",
    "        \"S0\":    _first(d, \"S0\", \"S0_list\"),\n",
    "        \"v0\":    _first(d, \"v0\", \"v0_list\"),\n",
    "        \"r\":     _first(d, \"r\", \"r_list\"),\n",
    "        \"q\":     _first(d, \"q\", \"q_list\"),\n",
    "        \"kappa\": _first(d, \"kappa\", \"kappa_list\"),\n",
    "        \"theta\": _first(d, \"theta\", \"theta_list\"),\n",
    "        \"sigma\": _first(d, \"sigma\", \"sigma_list\"),\n",
    "        \"rho\":   _first(d, \"rho\", \"rho_list\"),\n",
    "        \"xi_s\":  _first(d, \"xi_s\", \"xi_s_list\", 0.0),\n",
    "    }\n",
    "\n",
    "def _normalize_bates(d: dict) -> dict:\n",
    "    p = _normalize_heston(d)\n",
    "    p.update({\n",
    "        \"lambda_j\": _first(d, \"lambda_j\", default=0.0),\n",
    "        \"mu_j\": _first(d, \"mu_j\", default=0.0),\n",
    "        \"sigma_j\": _first(d, \"sigma_j\", default=0.0),\n",
    "        \"E_J_minus_1\": _first(d, \"E_J_minus_1\", default=0.0),\n",
    "        \"drift_adjustment\": _first(d, \"drift_adjustment\", default=0.0),\n",
    "    })\n",
    "    return p\n",
    "\n",
    "def build_real_case(heston_json=\"oneDheston.json\", bates_json=\"oneDbates.json\"):\n",
    "    h_raw = _load_json(heston_json) if os.path.exists(heston_json) else {}\n",
    "    b_raw = _load_json(bates_json) if os.path.exists(bates_json) else {}\n",
    "    h = _normalize_heston(h_raw) if h_raw else None\n",
    "    b = _normalize_bates(b_raw) if b_raw else None\n",
    "    S0 = (h or b)[\"S0\"]\n",
    "    V0 = (h or b)[\"v0\"]\n",
    "    r, q = (h or b)[\"r\"], (h or b)[\"q\"]\n",
    "    # real-data config\n",
    "    T = 59.0 / 252.0\n",
    "    n_steps = 59\n",
    "    Ks = np.array([3225.0, 2600.0], dtype=float)\n",
    "    kinds = [\"call\", \"put\"]\n",
    "    return (h, b, dict(S0=S0, V0=V0, r=r, q=q, T=T, n_steps=n_steps, Ks=Ks, kinds=kinds))\n",
    "\n",
    "\n",
    "# ----------------------------- core hedger -----------------------------------\n",
    "\n",
    "@dataclass\n",
    "class SimpleBench:\n",
    "    name: str\n",
    "    model: object  # HestonModel or BatesModel\n",
    "    r: float\n",
    "    q: float\n",
    "\n",
    "    # ---------- simulation wrapper (returns discounted X, dX and discounted payoffs) ----------\n",
    "    def _sim_discounted(self, measure: str, S0: float, V0: float, T: float, n_steps: int,\n",
    "                        n_paths: int, Ks: np.ndarray, kinds: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        dt = T / n_steps\n",
    "        if measure == \"mmm\":\n",
    "            S, V, _ = self.model.simulate_mmm(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        elif measure == \"vomm\":\n",
    "            S, V, _ = self.model.simulate_vomm(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        elif measure == \"p\":\n",
    "            S, V, _ = self.model.simulate_under_P(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        else:\n",
    "            raise ValueError(\"measure must be 'mmm', 'vomm', or 'p'\")\n",
    "        # Normalize simulated prices by S0\n",
    "        S = S / S0\n",
    "        t, disc = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        X = (S * disc[None, :]).astype(np.float64)          # discounted price paths\n",
    "        dX = X[:, 1:] - X[:, :-1]                           # discounted increments\n",
    "        H_tilde = _batch_discounted_payoffs(S[:, -1], Ks, kinds, disc[-1], S0)\n",
    "        return X, dX, H_tilde\n",
    "\n",
    "    # ---------- LSM helpers (basis + continuation values) ----------\n",
    "    def _poly_features(self, Xk: np.ndarray, Vk: Optional[np.ndarray], tk: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Build simple polynomial basis in discounted (normalized) state at time k.\n",
    "        Xk: (n_paths,) discounted normalized price; Vk: (n_paths,) variance (can be None); tk: scalar in [0, T].\n",
    "        \"\"\"\n",
    "        if Vk is None:\n",
    "            return np.column_stack([\n",
    "                np.ones_like(Xk),\n",
    "                Xk, Xk**2,\n",
    "                tk*np.ones_like(Xk),\n",
    "                tk*Xk\n",
    "            ]).astype(np.float64)\n",
    "        return np.column_stack([\n",
    "            np.ones_like(Xk),\n",
    "            Xk, Vk,\n",
    "            Xk**2, Vk**2, Xk*Vk,\n",
    "            tk*np.ones_like(Xk),\n",
    "            tk*Xk, tk*Vk\n",
    "        ]).astype(np.float64)\n",
    "\n",
    "    def _cond_mean_var(self, dX_col: np.ndarray, Xk_col: np.ndarray, Vk_col: Optional[np.ndarray], tk: float) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Conditional mean/variance of ΔX_k given F_k via polynomial regression.\n",
    "        Returns:\n",
    "          mu_hat: pathwise conditional mean (n_paths,)\n",
    "          var_hat: scalar conditional variance estimate\n",
    "        \"\"\"\n",
    "        Phi = self._poly_features(Xk_col, (Vk_col if Vk_col is not None else None), tk)\n",
    "        A = Phi.T @ Phi\n",
    "        A.flat[::A.shape[0]+1] += 1e-10\n",
    "        beta = np.linalg.solve(A, Phi.T @ dX_col)\n",
    "        mu_hat = Phi @ beta\n",
    "        resid = dX_col - mu_hat\n",
    "        var_hat = float(np.mean(resid * resid))\n",
    "        return mu_hat, var_hat\n",
    "\n",
    "    def _discount_normalize_from_S(self, S: np.ndarray, S0: float, T: float, n_steps: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Given raw S paths (n_paths, n_steps+1), return (X, dX, disc) with\n",
    "        X_t = e^{-(r-q)t} S_t / S0 and dX = ΔX.\"\"\"\n",
    "        t, disc = _disc_grid(self.r, self.q, T, n_steps)\n",
    "        S_norm = S / S0\n",
    "        X = (S_norm * disc[None, :]).astype(np.float64)\n",
    "        dX = X[:, 1:] - X[:, :-1]\n",
    "        return X, dX, disc\n",
    "     \n",
    "    def _apply_betas(self, X: np.ndarray, V: Optional[np.ndarray], t_grid: np.ndarray, betas: List[np.ndarray]) -> np.ndarray:\n",
    "      n_paths, T_plus_1 = X.shape\n",
    "      Vhat = np.zeros((n_paths, T_plus_1), dtype=np.float64)\n",
    "      for k in range(T_plus_1 - 1):\n",
    "         Xk = X[:, k]\n",
    "         Vk = None if V is None else V[:, k]\n",
    "         Phi = self._poly_features(Xk, Vk, t_grid[k])\n",
    "         Vhat[:, k] = Phi @ betas[k]\n",
    "      # terminal left at 0; we only need k<T values for dV etc.\n",
    "      return Vhat\n",
    "    \n",
    "    def _lsm_continuation_values(self, X: np.ndarray, V: Optional[np.ndarray],\n",
    "                             H_tilde_port: np.ndarray, t_grid: np.ndarray) -> Tuple[np.ndarray, List[np.ndarray]]:\n",
    "         n_paths, T_plus_1 = X.shape\n",
    "         Vhat = np.zeros((n_paths, T_plus_1), dtype=np.float64)\n",
    "         betas: List[np.ndarray] = [None]*(T_plus_1)  # store beta per time\n",
    "\n",
    "         Vhat[:, -1] = H_tilde_port\n",
    "         ridge = 1e-10\n",
    "\n",
    "         for k in range(T_plus_1 - 1):\n",
    "            Xk = X[:, k]\n",
    "            Vk = None if V is None else V[:, k]\n",
    "            Phi = self._poly_features(Xk, Vk, t_grid[k])\n",
    "            A = Phi.T @ Phi\n",
    "            A.flat[::A.shape[0]+1] += ridge\n",
    "            beta = np.linalg.solve(A, Phi.T @ H_tilde_port)\n",
    "            Vhat[:, k] = Phi @ beta\n",
    "            betas[k] = beta\n",
    "\n",
    "         betas[-1] = np.zeros_like(betas[0])  # terminal dummy\n",
    "         return Vhat, betas\n",
    "\n",
    "    # ---------- LRM theta under MMM (local projection on ΔX without drift-removal) ----------\n",
    "    def train_theta_lrm(self, S0, V0, T, n_steps, n_paths, Ks, kinds):\n",
    "         X, dX, H_tilde_mat = self._sim_discounted(\"mmm\", S0, V0, T, n_steps, n_paths, Ks, kinds)\n",
    "         H_port = H_tilde_mat.sum(axis=0)\n",
    "         t_grid, _ = _disc_grid(self.r, self.q, T, n_steps)\n",
    "         Vhat, betas = self._lsm_continuation_values(X, None, H_port, t_grid)\n",
    "\n",
    "         dV = Vhat[:, 1:] - Vhat[:, :-1]\n",
    "         theta = np.zeros(dX.shape[1], dtype=np.float64)\n",
    "         eps = 1e-12\n",
    "         for k in range(dX.shape[1]):\n",
    "            x, y = dX[:, k], dV[:, k]\n",
    "            cov = ((x - x.mean()) * (y - y.mean())).mean()\n",
    "            var = ((x - x.mean())**2).mean() + eps\n",
    "            theta[k] = cov / var\n",
    "\n",
    "         H0_lrm = float(np.mean(H_port))  # E^{MMM}[H~]\n",
    "         return theta, betas, H0_lrm\n",
    "\n",
    "    # ---------- MVH theta under VOMM (GKW integrand via LSM; v0 handled at evaluation) ----------\n",
    "    def train_theta_mvh(self, S0, V0, T, n_steps, n_paths, Ks, kinds):\n",
    "         have_with_density = hasattr(self.model, 'simulate_vomm_with_density')\n",
    "         if have_with_density:\n",
    "            S, V, Z, comp_ratio = self.model.simulate_vomm_with_density(S0, V0, T, T/n_steps, n_paths, _show_progress=False)\n",
    "         else:\n",
    "            S, V, _ = self.model.simulate_vomm(S0, V0, T, T/n_steps, n_paths, _show_progress=False)\n",
    "            Z = None; comp_ratio = None\n",
    "\n",
    "         X, dX, disc = self._discount_normalize_from_S(S, S0, T, n_steps)\n",
    "         S_T_norm = (S / S0)[:, -1]\n",
    "         H_tilde_mat = _batch_discounted_payoffs(S_T_norm, Ks, kinds, disc[-1], S0=S0)\n",
    "         H_port = H_tilde_mat.sum(axis=0)\n",
    "\n",
    "         t_grid, _ = _disc_grid(self.r, self.q, T, n_steps)\n",
    "         Vhat, betas = self._lsm_continuation_values(X, None, H_port, t_grid)\n",
    "         dV = Vhat[:, 1:] - Vhat[:, :-1]\n",
    "\n",
    "         Tm1 = dX.shape[1]\n",
    "         xi = np.zeros(Tm1, dtype=np.float64)\n",
    "         eps = 1e-12\n",
    "         for k in range(Tm1):\n",
    "            x, y = dX[:, k], dV[:, k]\n",
    "            cov = ((x - x.mean()) * (y - y.mean())).mean()\n",
    "            var = ((x - x.mean())**2).mean() + eps\n",
    "            xi[k] = cov / var\n",
    "\n",
    "         H0_mvh = float(np.mean(H_port))  # E^{VOMM}[H~]\n",
    "         return xi, betas, H0_mvh, comp_ratio\n",
    "\n",
    "    # ---------- evaluate under P (portfolio = CALL+PUT) ----------\n",
    "    def evaluate_P(self,\n",
    "               xi_mvh: np.ndarray, betas_vomm: List[np.ndarray], H0_mvh: float,\n",
    "               theta_lrm: np.ndarray, betas_mmm: List[np.ndarray], H0_lrm: float,\n",
    "               S0: float, V0: float, T: float, n_steps: int, n_paths: int,\n",
    "               Ks: np.ndarray, kinds: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Paper-faithful P-evaluation (discounted, normalized).\n",
    "        - MVH error:  H~ - (H0_mvh + sum θ^{MVH}_k(path) ΔX_k)  with feedback (Eq. 4.24)\n",
    "        - LRM error:  H~ - (H0_lrm + sum θ^{LRM}_k ΔM_k)        with ΔM_k = ΔX_k - E[ΔX_k|F_k]\n",
    "        Returns orthogonality metrics for both MVH and LRM.\n",
    "        \"\"\"\n",
    "        dt = T / n_steps\n",
    "\n",
    "        # Simulate under P and build discounted normalized state\n",
    "        S, V, _ = self.model.simulate_under_P(S0, V0, T, dt, n_paths, _show_progress=False)\n",
    "        X, dX, disc = self._discount_normalize_from_S(S, S0, T, n_steps)\n",
    "\n",
    "        # Discounted payoff (portfolio = sum over options), already normalized by S0 in payoff helper\n",
    "        S_T_norm = (S / S0)[:, -1]\n",
    "        H_tilde_mat = _batch_discounted_payoffs(S_T_norm, Ks, kinds, disc[-1], S0=S0)\n",
    "        payoff_port = H_tilde_mat.sum(axis=0)  # (n_paths,)\n",
    "\n",
    "        T_al = min(dX.shape[1], xi_mvh.shape[0], theta_lrm.shape[0])\n",
    "        dX = dX[:, :T_al]\n",
    "        Xk = X[:, :T_al]              # state X at begins of steps\n",
    "        Vk = V[:, :T_al] if V is not None else None\n",
    "        t_grid, _ = _disc_grid(self.r, self.q, T, n_steps)\n",
    "\n",
    "        # ---------- LRM: build ΔM_k via conditional mean regression per time ----------\n",
    "        Phi_list = []\n",
    "        for k in range(T_al):\n",
    "            phi = np.column_stack([\n",
    "                np.ones_like(Xk[:, k]), Xk[:, k],\n",
    "                (Vk[:, k] if Vk is not None else Xk[:, k]*0.0),\n",
    "                Xk[:, k]**2,\n",
    "                (Xk[:, k]*(Vk[:, k] if Vk is not None else 0.0)),\n",
    "                (Vk[:, k] if Vk is not None else 0.0)**2\n",
    "            ]).astype(np.float64)\n",
    "            Phi_list.append(phi)\n",
    "        dM = np.empty_like(dX)\n",
    "        mu_cond = np.empty_like(dX)\n",
    "        for k in range(T_al):\n",
    "            A = Phi_list[k].T @ Phi_list[k]\n",
    "            A.flat[::A.shape[0]+1] += 1e-10\n",
    "            beta_mu = np.linalg.solve(A, Phi_list[k].T @ dX[:, k])\n",
    "            mu_hat = Phi_list[k] @ beta_mu                  # ≈ E_P[ΔX_k | F_k]\n",
    "            mu_cond[:, k] = mu_hat\n",
    "            dM[:, k] = dX[:, k] - mu_hat\n",
    "\n",
    "        # LRM: pathwise gains and error\n",
    "        gains_lrm = (dM * theta_lrm[:T_al][None, :]).sum(axis=1)\n",
    "        err_lrm = payoff_port - (H0_lrm + gains_lrm)       # equals L_T (discrete)\n",
    "\n",
    "        # ---------- LRM orthogonality: E[L_T * M_T] with M_T = Σ θ_k ΔM_k (should be ~0) ----------\n",
    "        # For LRM, orthogonality: E[L_T * M_T] where M_T = sum θ_k dM_k\n",
    "        M_T_lrm = gains_lrm\n",
    "        L_T_lrm = err_lrm\n",
    "        ortho_lrm = float(np.mean(L_T_lrm * M_T_lrm))\n",
    "\n",
    "        # ---------- MVH: feedback using conditional regressions (stable scalar α_k) ----------\n",
    "        # Reconstruct V^{H,~P}_k along P-paths using VOMM betas\n",
    "        Vhat_vomm = self._apply_betas(X[:, :T_al+1], None, t_grid, betas_vomm)\n",
    "\n",
    "        # Compute scalar α_k = E[μ_k] / Var_k with μ_k, Var_k from conditional regressions at each time\n",
    "        alpha_scalar = np.zeros(T_al, dtype=np.float64)\n",
    "        for k in range(T_al):\n",
    "            mu_hat_k, var_hat_k = self._cond_mean_var(dX[:, k], Xk[:, k], (Vk[:, k] if Vk is not None else None), t_grid[k])\n",
    "            # baseline conditional mean (diffusion part)\n",
    "            num = float(np.mean(mu_hat_k))\n",
    "\n",
    "            # --- Bates jump drift correction under P: add λ * E[J-1] * E[X_k] * Δt ---\n",
    "            if isinstance(self.model, BatesModel):\n",
    "                # Prefer explicit E[J-1] if provided; else compute from (mu_j, sigma_j)\n",
    "                EJm1 = getattr(self.model, 'E_J_minus_1', None)\n",
    "                if EJm1 is None:\n",
    "                    mu_j = getattr(self.model, 'mu_j', 0.0)\n",
    "                    sigma_j = getattr(self.model, 'sigma_j', 0.0)\n",
    "                    EJm1 = np.exp(mu_j + 0.5 * sigma_j * sigma_j) - 1.0\n",
    "                lam = getattr(self.model, 'lambda_j', 0.0)\n",
    "                jd = lam * EJm1 * float(np.mean(Xk[:, k])) * dt\n",
    "                num += jd\n",
    "\n",
    "            den = max(var_hat_k, 1e-12)\n",
    "\n",
    "            a = num / den\n",
    "            # Clamp α_k conservatively; tighter for Bates to avoid recursion blow-up due to sparse jumps\n",
    "            if isinstance(self.model, BatesModel):\n",
    "                alpha_scalar[k] = np.clip(a, -10.0, 10.0)\n",
    "            else:\n",
    "                alpha_scalar[k] = np.clip(a, -100.0, 100.0)\n",
    "\n",
    "        # Recursion: θ^{MVH}_k(path) = ξ_k + α_k * ( Vhat_k - H0_mvh - sum_{j<k} θ_j ΔX_j )\n",
    "        theta_path = np.zeros_like(dX)\n",
    "        cum = np.zeros(dX.shape[0], dtype=np.float64)   # cumulative gains up to k-1\n",
    "        # Tighter clips for Bates (jumps); looser for Heston\n",
    "        if isinstance(self.model, BatesModel):\n",
    "            theta_clip = 10.0\n",
    "            cum_clip = 2.0\n",
    "        else:\n",
    "            theta_clip = 50.0\n",
    "            cum_clip = 10.0\n",
    "        for k in range(T_al):\n",
    "            Vk_meanless = Vhat_vomm[:, k] - H0_mvh\n",
    "            raw_theta = xi_mvh[k] + alpha_scalar[k] * (Vk_meanless - cum)\n",
    "            raw_theta = np.clip(raw_theta, -theta_clip, theta_clip)\n",
    "            theta_path[:, k] = raw_theta\n",
    "            cum = np.clip(cum + raw_theta * dX[:, k], -cum_clip, cum_clip)\n",
    "\n",
    "        gains_mvh = (theta_path * dX).sum(axis=1)\n",
    "        # MVH error equals cost martingale \\tilde L_T under P (HPS Eq.4.24 discrete analogue)\n",
    "        err_mvh = payoff_port - (H0_mvh + gains_mvh)\n",
    "\n",
    "        # MVH orthogonality w.r.t. martingale part M_T = Σ θ_k ΔM_k under P\n",
    "        M_T_mvh = (dM * theta_path).sum(axis=1)  # reuse dM from LRM construction above\n",
    "        L_T_mvh = err_mvh\n",
    "        ortho_mvh = float(np.mean(L_T_mvh * M_T_mvh))\n",
    "\n",
    "        # ---------- metrics ----------\n",
    "        def _metrics(e: np.ndarray, unhedged: np.ndarray) -> Dict:\n",
    "            mean = float(np.mean(e))\n",
    "            mse = float(np.mean(e**2))\n",
    "            var = max(mse - mean**2, 0.0)\n",
    "            rmse = float(np.sqrt(mse))\n",
    "            std = float(np.sqrt(var))\n",
    "            heff = 1.0 - (var / max(float(np.var(unhedged)), 1e-12))\n",
    "            return dict(MSE=mse, RMSE=rmse, Mean=mean, Std=std, HedgeEffPct=100.0*heff)\n",
    "\n",
    "        mv = _metrics(err_mvh, payoff_port)\n",
    "        lr = _metrics(err_lrm, payoff_port)\n",
    "\n",
    "        return dict(\n",
    "            measure=\"P\",\n",
    "            v0_mvh_disc=float(H0_mvh),\n",
    "            H0_lrm_disc=float(H0_lrm),\n",
    "            mv_metrics=mv,\n",
    "            lr_metrics=lr,\n",
    "            theta_mvh_port=None,  # pathwise; too big to dump by default\n",
    "            theta_lrm_port=theta_lrm[:T_al].tolist(),\n",
    "            # orthogonality_mv=float(ortho_mvh),\n",
    "            orthogonality_lrm=float(ortho_lrm),\n",
    "        )\n",
    "\n",
    "# ------------------------------- runner --------------------------------------\n",
    "\n",
    "def run_realdata():\n",
    "    # load calibrated params and real case\n",
    "    h_p, b_p, case = build_real_case()\n",
    "    S0, V0, r, q, T, n_steps, Ks, kinds = case[\"S0\"], case[\"V0\"], case[\"r\"], case[\"q\"], case[\"T\"], case[\"n_steps\"], case[\"Ks\"], case[\"kinds\"]\n",
    "\n",
    "\n",
    "    # models\n",
    "    heston = HestonModel({'kappa': h_p['kappa'], 'theta': h_p['theta'], 'sigma': h_p['sigma'], 'rho': h_p['rho'],\n",
    "                          'v0': h_p['v0'], 'r': h_p['r'], 'q': h_p['q'], 'xi_s': h_p.get('xi_s', 0.0)})\n",
    "    if b_p is not None and all(k in b_p for k in ['lambda_j','mu_j','sigma_j']):\n",
    "        bates = BatesModel({'kappa': b_p['kappa'], 'theta': b_p['theta'], 'sigma': b_p['sigma'], 'rho': b_p['rho'],\n",
    "                            'v0': b_p['v0'], 'r': b_p['r'], 'q': b_p['q'], 'xi_s': b_p.get('xi_s', 0.0),\n",
    "                            'lambda_j': b_p['lambda_j'], 'mu_j': b_p['mu_j'], 'sigma_j': b_p['sigma_j'],\n",
    "                            'E_J_minus_1': b_p.get('E_J_minus_1', 0.0), 'drift_adjustment': b_p.get('drift_adjustment', 0.0)})\n",
    "    else:\n",
    "        bates = BatesModel({'kappa': h_p['kappa'], 'theta': h_p['theta'], 'sigma': h_p['sigma'], 'rho': h_p['rho'],\n",
    "                            'v0': h_p['v0'], 'r': h_p['r'], 'q': h_p['q'], 'xi_s': h_p.get('xi_s', 0.0),\n",
    "                            'lambda_j': 0.0, 'mu_j': 0.0, 'sigma_j': 0.0, 'E_J_minus_1': 0.0, 'drift_adjustment': 0.0})\n",
    "\n",
    "    # simple benches\n",
    "    hb = SimpleBench(\"Heston\", heston, r, q)\n",
    "    bb = SimpleBench(\"Bates\", bates, r, q)\n",
    "\n",
    "    # path budgets (powers of two)\n",
    "    n_train = _next_pow2(2**18)        # 262,144 (validation-speed)\n",
    "    n_eval  = _next_pow2(2**17)        # 131,072 (evaluation under P)\n",
    "\n",
    "    # train thetas (portfolio-level)\n",
    "    xi_lrm_h, betas_mmm_h, H0_lrm_h = hb.train_theta_lrm(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "    xi_mvh_h, betas_vomm_h, H0_mvh_h, comp_h = hb.train_theta_mvh(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "\n",
    "    xi_lrm_b, betas_mmm_b, H0_lrm_b = bb.train_theta_lrm(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "    xi_mvh_b, betas_vomm_b, H0_mvh_b, comp_b = bb.train_theta_mvh(S0, V0, T, n_steps, n_train, Ks, kinds)\n",
    "\n",
    "    # evaluate under P\n",
    "    res_h = hb.evaluate_P(xi_mvh_h, betas_vomm_h, H0_mvh_h,\n",
    "                      xi_lrm_h, betas_mmm_h, H0_lrm_h,\n",
    "                      S0, V0, T, n_steps, n_eval, Ks, kinds)\n",
    "    res_b = bb.evaluate_P(xi_mvh_b, betas_vomm_b, H0_mvh_b,\n",
    "                      xi_lrm_b, betas_mmm_b, H0_lrm_b,\n",
    "                      S0, V0, T, n_steps, n_eval, Ks, kinds)\n",
    "\n",
    "    # save artifacts\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    outdir = f\"artifacts/benchmark_eval/{ts}/\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    with open(os.path.join(outdir, \"heston_eval_P.json\"), \"w\") as f:\n",
    "        json.dump(res_h, f, indent=2)\n",
    "    with open(os.path.join(outdir, \"bates_eval_P.json\"), \"w\") as f:\n",
    "        json.dump(res_b, f, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved P-eval artifacts to {outdir}\")\n",
    "    print(f\"Heston (portfolio) MV RMSE={res_h['mv_metrics']['RMSE']:.6f}  |  LRM RMSE={res_h['lr_metrics']['RMSE']:.6f}\")\n",
    "    print(f\"Bates  (portfolio) MV RMSE={res_b['mv_metrics']['RMSE']:.6f}  |  LRM RMSE={res_b['lr_metrics']['RMSE']:.6f}\")\n",
    "\n",
    "    return dict(heston=res_h, bates=res_b, outdir=outdir)\n",
    "\n",
    "\n",
    "# ------------------------------ entry point ----------------------------------\n",
    "\n",
    "def runner():\n",
    "    return run_realdata()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c953839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
